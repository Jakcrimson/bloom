from huggingface_hub import InferenceApi
from IPython.display import HTML as html_print
import streamlit as st
from PIL import Image

inference = InferenceApi("bigscience/bloom",token="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")



def cstr(s, color='black'):
    #return "<text style=color:{}>{}</text>".format(color, s)
    return "<text style=color:{}>{}</text>".format(color, s.replace('\n', '<br>'))

def cstr_with_newlines(s, color='black'):
    return "<text style=color:{}>{}</text>".format(color, s.replace('\n', '<br>'))


def infer(prompt,
          max_length = 200,
          top_k = 0,
          num_beams = 0,
          no_repeat_ngram_size = 2,
          top_p = 0.9,
          seed=42,
          temperature=0.7,
          greedy_decoding = False,
          return_full_text = False):
    

    top_k = None if top_k == 0 else top_k
    do_sample = False if num_beams > 0 else not greedy_decoding
    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams
    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size
    top_p = None if num_beams else top_p
    early_stopping = None if num_beams is None else num_beams > 0

    params = {
        "max_new_tokens": max_length,
        "top_k": top_k,
        "top_p": top_p,
        "temperature": temperature,
        "do_sample": do_sample,
        "seed": seed,
        "early_stopping":early_stopping,
        "no_repeat_ngram_size":no_repeat_ngram_size,
        "num_beams":num_beams,
        "return_full_text":return_full_text
    }
    
    response = inference(prompt, params=params)
    return html_print(cstr(response[0]['generated_text'], color='#a1d8eb')), response[0]['generated_text']


## Content of the APP

st.title("Chat with BLOOM ^^")

## Description of the project
st.markdown("""
The official documentation for the :hibiscus:**BLOOM**:hibiscus: project can be found here : **https://huggingface.co/bigscience/bloom**""")

st.header("Short Introduction to the project")

pie_chart = Image.open('./assets/pie_v2.png')

st.markdown("""#### Model Details :wrench: 
:hibiscus:BLOOM:hibiscus: is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.

### Specs :

- :computer:**Model Type** : Transformed-based Language Model
- :bar_chart:**Training Data** : Training data includes:

    - 46 natural languages

    - 13 programming languages

    - In 1.6TB of pre-processed text, converted into 350B unique tokens (see the tokenizer section for more.) 
""")

st.image(pie_chart, caption='Distribution of languages in training data data')

st.markdown("""
- :book:**Tokenization** : The BLOOM tokenizer (link), a learned subword tokenizer trained using:

    - A byte-level Byte Pair Encoding (BPE) algorithm

    - A simple pre-tokenization rule, no normalization

    - A vocabulary size of 250,680

- :earth_americas:**Environmental Impact** : The training supercomputer, Jean Zay (website), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.

- :credit_card:**Funded by** :

    - The French government (:sunglasses:).

    - Hugging Face (website).

    - Organizations of contributors. (Further breakdown of organizations forthcoming.)

""")

st.markdown("""### Using the :hibiscus:BLOOM:hibiscus: API

As specified above, the model in it's entirety weighs around **350 Go**(:flushed:). So BigScience made an API for people with not a lot of coputing power to use the model.
This API is very easy to use and provides very interesting results.

Before I give a few examples it's important to know that in order to have interesting results, the model must be provided with context and some "ideas" of what he's supposed to do.
The more complex the the context or the idea will be, the more interesting the reponse will be.""")

st.markdown("""### A Few Examples

#### Giving context information:

A way of giving context to the model is by specifying it in such way that it will understand it's a context:
""")

st.caption("""
**Context:** 
A person is at a supermarket and cannot figure out where the fruits are. It asks an AI where they are but the AI is very reluctant and does not wish to answer. The AI's manager steps in and shouts on the AI.

**Questions:**
1. How do you think the AI will react ?
2. How should the manager have reacted ?
3. How do you think the person will react if the AI decides to hit the manager ?

**Answers:**
1.
""")

st.markdown(""" In the previous example, i gave the model clear indications of what i wante and how it should be structured. This way, the model has a guideline. I've asked open questions as well. Let's see the results :
""")

st.caption("""
Answers:
1. The AI will say, "Where are the fruits?"
2. The manager should have said, "Do you not understand English?"
3. The person will get very angry and ask the manager to leave.
4. The AI does not wish to answer because it has been asked to do so many times and it is tired of doing so.
""")
st.markdown("Another example:")
st.caption("""
Context : A very angry fruit seller is telling people that his fruits are the best. However no one will buy him anything because he is too agressive.

Questions:
1. How should the fruit seller behave in order to sell more fruits?
2. What are the odds that someone buys him anything?
3. What is your personal opinion on this situation?

Answers:
1. He should be more polite.
2. The odds are 0.3 %
3. I think that the fruit seller should behave in a more polite way, otherwise no one will buy him anything.
""")

st.markdown(""" Use the previous example and paste it in the text area. The results will most likely change for each little thing you change.""")
st.markdown("""### Use :hibiscus:BLOOM:hibiscus: """)
user_input = st.text_area("Enter a question, a sentence, a complex input...")


if user_input:
    color_resp, resp = infer(user_input)
    st.write(color_resp)
else:
    pass
